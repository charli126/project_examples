---
title: "Modeling and prediction for movies"
author: "Xia Cui"
date: "Sunday, October 4, 2020"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
```

### Load data

```{r load-data}
load("movies.Rdata")
```
 
* * *

## Part 1: Data

The data set includes information from Rotten Tomatoes and IMDB for a randomly sampled 651 movies produced and released before 2016, showing their information about 32 variables. Since a random sampling method is used, any relationship found or models built based on the analysis of the sample can be generalized to other movies with caution. To avoid extrapolation, those who want to generalize the findings need to check if the the movies of interest are within the the scope of the movies in the sample. As the data is not from an experimentation study, no causal links should be made among the variables. 

* * *

## Part 2: Research question

The analysis in this report will address one question: movies with which attributes are more popular? In order to achieve this, a multiple linear regression model will be built to find out the significant predictors of movie popularity. Such information could potentially be useful for movie producers and promoting companies. 

* * *

## Part 3: Exploratory data analysis

To gather the information needed for the task, we'll first examine the popularity of the movies in the sample as measured by two variables: audience_score, and imdb_rating. Both are numerical variables. According to the imdb website, imdb rating for a movie is aggregated and summarized from individual votes by registered users from 1 to 10. According to Rotten Tomato website, an audience score is calculated from ratings submitted to Rotten Tomatoes by users. It is the percentage of users who have rated the movie or TV shows positively. When at least 60% of users give a movie or TV show a star rating of 3.5 or higher, the movie or TV show will have audience rating of upright, whereas lower than 60% will entail a Spilled status. The audience_score in the data set are shown in numbers between 1 and 100, indicating percentages.  

```{r}
#summary of imdb_rating of the sampled movies
summary(movies$imdb_rating)
boxplot(movies$imdb_rating, horizontal=TRUE)
text(x = boxplot.stats(movies$imdb_rating)$stats, labels = boxplot.stats(movies$imdb_rating)$stats, y=1.25)
#count the number of outliers (imdb_rating<1st quantitle-1.5*sd)
movies%>%filter(imdb_rating<4.28)%>%count()
```

As shown in the boxplot above, the minimum score for the imdb rating is 1.9, and maximum is 9. 50% of the rating is between 5.9 to 7.3.The distribution of the scores is left skewed, centred at 6.493. There are 28 outliers with extremely low score.

```{r}
#summary of the audience_score of the sampled movies
summary(movies$audience_score)
boxplot(movies$audience_score, horizontal=TRUE)
text(x=boxplot.stats(movies$audience_score)$stats, labels=boxplot.stats(movies$audience_score)$stats, y=1.25)
```


As shown in the boxplot above, the audience_score for the sampled movies range from 11 to 97, wiht 50% of the movies scored between 46 to 80. The distribution of the score is slightly left skewed with a mean score of 62.36.  

Although imdb_rating and audience_score are both measures of a movie's popularity, what we don't know is whether people visiting the two websites rate movies differently. To find out the relationship between the two popularity measurements, a linear regression model with the audience_score as the response variable, and the imdb_rating as the explanatory variable is fit. 


```{r}
ggplot(data=movies, aes(x=imdb_rating, y=audience_score))+
  geom_jitter()+
  stat_smooth(method="lm", se=F)
```

Judging from the graph above, there is a strong, positive, and possibly linear relationship between the two variables. The histogram of the the residuals below shows that the distribution is centered at 0 but not exactly normal. The residuals vs.imdb_rating plot shows the residuals randomly scatter around 0, however, the residuals vs. fitted value plot shows that the variability is not exactly constant. Therefore it reasonable to question the reliability of using a simple linear model to predict audience_score based on imdb_rating. 


```{r}
imdb_tomato<-lm(movies$audience_score~movies$imdb_rating)
summary(imdb_tomato)
#histogram of the residuals
hist(imdb_tomato$residuals)
```
```{r}
#plot the residuals vs. imdb_rating
plot(imdb_tomato$residuals, imdb_tomato$imdb_rating)
```

```{r}
#plot the residuals vs. fitted
plot(imdb_tomato$fitted, imdb_tomato$residuals)
```



For the reason stated above, as well as the possilibity that the attributes accounting for the popularity of movies might differ between the two websites, two multiple linear models will be built to predict the popularity of movies, using audience_score and imdb_rating as the response variable respectively. 

* * *

## Part 4: Modeling


To fit the best models, the backward elimination method is selected, starting with a full model and drop one predictor at a time until the parsimonious model is reached. The Pvalue approach is used for this process because the task is to understand which variables are statistically significant predictors of the movie's popularity. 

First we need to decide which variables in the data will be excluded from the initial models. 

For both models, the following variables are excluded. 

1. title, because titleis a standalone attribute of the movie. 

2. the theaters and dvd release year is excluded, and the month and day are kept. This is because the release month and day might affect how many people have watched and rated the movies. 

3. critics_rating: critics score is kept instead because the rating is based on the score. 

4. audience_rating: it is based on the audience_rating that the audience_score is generated and therefore it is repetitive.  

6. director and actors: although movies from certain directors or having certain cast might appeal to audience more and therefore be more popular, these are standalone features of the movies and vary movie by movie.

7. the imdb_url and rt_url. These are links to websites unique to each movies. 

8.studio. There are a total of 211 studios, simply too many to take into account if the goal is to build the simplest model with the highest prediction power. 

For the first model using audience_score as the response variable, imdb_num_votes is also excluded because this is a unique feature to the imdb_rating. 

After the selection above, the variables used to build the first multiple linear regression model are: title_type, genre, runtime, mpaa_rating, studio, thtr_rel_month, thtr_rel_day, dvd_rel_month, dvd_rel_day, critics_score, best_pic_nom, best_pic_win, best_actor_win, best_actress_win, best_dir_win, and top200_box, a total of 16 variables, in addition to the response variable audience_score.

The variabls used to build the second model are: title_type, genre, runtime, mpaa_rating, studio, thtr_rel_month, thtr_rel_day, dvd_rel_month, dvd_rel_day, critics_score, imdb_num_votes, best_pic_nom, best_pic_win, best_actor_win, best_actress_win, best_dir_win, and top200_box, a total of 17 variables, in addition to the response variable imdb_rating.

Using the backward elimination method, a full model for model 1 is built.

```{r}
mlr_full<-lm(audience_score ~ title_type + genre + runtime + 
               mpaa_rating + thtr_rel_month + thtr_rel_day + 
               dvd_rel_month + dvd_rel_day + critics_score + 
               best_pic_nom + best_pic_win + best_actor_win + 
               best_actress_win + best_dir_win + top200_box, data=movies)
summary(mlr_full)
```

```{r}
#drop the best director win which has the highest pvalue. 
mlr_1<-lm(audience_score ~ title_type + genre + runtime + 
            mpaa_rating + thtr_rel_month + thtr_rel_day + 
            dvd_rel_month + dvd_rel_day + critics_score + 
            best_pic_nom + best_pic_win + best_actor_win + 
            best_actress_win + top200_box, data=movies)
summary(mlr_1)
```


```{r}
#The next highest pvalue is title_typeFeature film. As the other level in title_type also has a relatively high pvalue, this variable is dropped next.

mlr_2<-lm(audience_score ~ genre + runtime + mpaa_rating + 
            thtr_rel_month + thtr_rel_day + dvd_rel_month + 
            dvd_rel_day + critics_score + best_pic_nom + 
            best_pic_win + best_actor_win + best_actress_win + 
            top200_box, data=movies)
summary(mlr_2)
```

```{r}
#mpaa_ratingR now has the highest pvalue. As the other levels in mpaa_rating also have high pvalues, next mpaa_rating is dropped. 
mlr_3<-lm(audience_score ~ genre + runtime + thtr_rel_month + 
            thtr_rel_day + dvd_rel_month + dvd_rel_day + 
            critics_score + best_pic_nom + best_pic_win + 
            best_actor_win + best_actress_win + top200_box, data=movies)
summary(mlr_3)
```

```{r}
#best picture win is dropped
mlr_4<-lm(audience_score ~ genre + runtime + thtr_rel_month + 
            thtr_rel_day + dvd_rel_month + dvd_rel_day + 
            critics_score + best_pic_nom + best_actor_win + 
            best_actress_win + top200_box, data=movies)
summary(mlr_4)
```


```{r}
#the theatre realse day is dropped
mlr_5<-lm(audience_score ~ genre + runtime + thtr_rel_month + 
            dvd_rel_month + dvd_rel_day + critics_score + 
            best_pic_nom + best_actor_win + best_actress_win + 
            top200_box, data=movies)
summary(mlr_5)
```


```{r}
#thtr_rel_month is dropped 
mlr_6<-lm(audience_score ~ genre + runtime + dvd_rel_month + 
            dvd_rel_day + critics_score + best_pic_nom + 
            best_actor_win + best_actress_win + top200_box, data=movies)
summary(mlr_6)
```


```{r}
#best_actor_win is dropped
mlr_7<-lm(audience_score ~ genre + runtime + dvd_rel_month + 
            dvd_rel_day + critics_score + best_pic_nom + 
            best_actress_win + top200_box, data=movies)
summary(mlr_7)
```


```{r}
#dvd release day is dropped
mlr_8<-lm(audience_score ~ genre + runtime + dvd_rel_month + 
            critics_score + best_pic_nom + best_actress_win + 
            top200_box, data=movies)
summary(mlr_8)
```

```{r}
#top 200 box is dropped
mlr_9<-lm(audience_score ~ genre + runtime + dvd_rel_month + 
            critics_score + best_pic_nom + best_actress_win, data=movies)
summary(mlr_9)
```


```{r}
#best_actress_win is dropped
mlr_10<-lm(audience_score ~ genre + runtime + dvd_rel_month + 
             critics_score + best_pic_nom, data=movies)
summary(mlr_10)
```

```{r}
#dvd_rel_month is dropped
mlr_11<-lm(audience_score ~ genre + runtime + critics_score + 
             best_pic_nom, data=movies)
summary(mlr_11)
```

```{r}
#runtime is dropped
mlr_fin<-lm(audience_score ~ genre + critics_score + best_pic_nom, data=movies)
summary(mlr_fin)
```


After the above elimination, now we have a model containing 3 predictor variables, among which genre has 11 levels. 3 of these 11 levels have pvalue smaller than 0.05, genre will stay in the final model. 

```{r}
levels(movies$genre)
```



Therefore the final multiple linear model to predict the audience_score of a movie consists of genre, critics_score, and best_pic_nom. The adjusted R squared is 0.5266. 

The equation for this regression model could be written as:

audience_score=35.375+4.75*genreAnimation+5.68*Genre Arthouse & International-1.17*GenreComedy + 9*genreDocumentary+1.77*genreDrama-9.08*GenreHorror+10.72*GenreMusical & performing arts-4.18*Genre Mystery & suspense+1.23*Genreother-6.70*genre Science Fiction & Fantasy+ 0.44*critics_score +10.04*best_pic_nomyes 

The reference level for genre is Action and adventure, and the referene level for best_pic_nom is no. 

In the context of the data, the model predicts that, all else held constant, a movie in the genre of horror is expected to have the lowest score on average compared with movies in other genres. The slope for best_pic_nomyes is 10.04, meaning that, all else held constant, the model predicts that a movie that has been nominated for an Oscar best picture on average is expected to have a score 10.4 higher those who haven't been. All else held constant, a movie that has an additional critics score is expected to have 0.44 additional audience score on average. 

Now let's fit the second model using imdb_rating as the response variable following a similar elimination process. 


```{r}
#full model
mlr2_full<-lm(imdb_rating ~ title_type + genre + runtime + 
                mpaa_rating + thtr_rel_month + thtr_rel_day + 
                dvd_rel_month + dvd_rel_day + critics_score + 
                imdb_num_votes + best_pic_nom + best_pic_win + 
                best_actor_win + best_actress_win + best_dir_win + 
                top200_box, data=movies)
summary(mlr2_full)
```

```{r}
#drop best_actress_win
mlr2_1<-lm(imdb_rating ~ title_type + genre + runtime + 
             mpaa_rating + thtr_rel_month + thtr_rel_day + 
             dvd_rel_month + dvd_rel_day + critics_score + 
             imdb_num_votes + best_pic_nom + best_pic_win + 
             best_actor_win + best_dir_win + top200_box, data=movies)
summary(mlr2_1)

```


```{r}
#drop best_actor_win
mlr2_2<-lm(imdb_rating ~ title_type + genre + runtime + 
             mpaa_rating + thtr_rel_month + thtr_rel_day + 
             dvd_rel_month + dvd_rel_day + critics_score + 
             imdb_num_votes + best_pic_nom + best_pic_win + 
             best_dir_win + top200_box, data=movies)
summary(mlr2_2)
```

```{r}
#drop best_director_win
mlr2_3<-lm(imdb_rating ~ title_type + genre + runtime + 
             mpaa_rating + thtr_rel_month + thtr_rel_day + 
             dvd_rel_month + dvd_rel_day + critics_score + 
             imdb_num_votes + best_pic_nom + best_pic_win + 
             top200_box, data=movies)
summary(mlr2_3)
```


```{r}
#drop theatre release day
mlr2_4<-lm(imdb_rating ~  title_type + genre + runtime + 
             mpaa_rating + thtr_rel_month + dvd_rel_month + 
             dvd_rel_day + critics_score + imdb_num_votes + 
             best_pic_nom + best_pic_win + top200_box, data=movies)
summary(mlr2_4)
```


```{r}
#drop title_type
mlr2_5<-lm(imdb_rating ~  + genre + runtime + mpaa_rating + 
             thtr_rel_month + dvd_rel_month + dvd_rel_day + 
             critics_score + imdb_num_votes + best_pic_nom + 
             best_pic_win + top200_box, data=movies)
summary(mlr2_5)
```


```{r}
#drop best pic nom
mlr2_6<-lm(imdb_rating ~  + genre + runtime + mpaa_rating + 
             thtr_rel_month + dvd_rel_month + dvd_rel_day + 
             critics_score + imdb_num_votes + best_pic_win + 
             top200_box, data=movies)
summary(mlr2_6)
```

```{r}
#drop theatre release month
mlr2_7<-lm(imdb_rating ~  + genre + runtime + mpaa_rating + 
             dvd_rel_month + dvd_rel_day + critics_score + 
             imdb_num_votes + best_pic_win + top200_box, data=movies)
summary(mlr2_7)
```


```{r}
#drop best pic win
mlr2_8<-lm(imdb_rating ~  + genre + runtime + mpaa_rating + 
             dvd_rel_month + dvd_rel_day + critics_score + 
             imdb_num_votes + top200_box, data=movies)
summary(mlr2_8)
```


```{r}
#as mpaa_ratingR has the next highest rating, and the other levels in mpaa_rating also have relatively high pvalue, this varialbe is dropped next
mlr2_9<-lm(imdb_rating ~  + genre + runtime + dvd_rel_month + 
             dvd_rel_day + critics_score + imdb_num_votes + 
             top200_box, data=movies)
summary(mlr2_9)
```


```{r}
#drop top200_box
mlr2_10<-lm(imdb_rating ~  + genre + runtime + dvd_rel_month + 
              dvd_rel_day + critics_score + imdb_num_votes, data=movies)
summary(mlr2_10)
```


```{r}
#drop dvd_release_day
mlr2_11<-lm(imdb_rating ~  + genre + runtime + dvd_rel_month + 
              critics_score + imdb_num_votes, data=movies)
summary(mlr2_11)
```


```{r}
#drop dvd_release_month
mlr2_fin<-lm(imdb_rating ~  + genre + runtime + critics_score + 
               imdb_num_votes, data=movies)
summary(mlr2_fin)
```


Now for imdb_rating, we have a multiple linear model of 4 explanatory variables: genre, runtime, critics_score, and imdb_num_votes. The variable genre has 11 levels, and 4 out of 10 have a pvalue smaller than 0.05. Therefore genre stays in the model. 

The reference level for genre is Action and adventure.The equation for the final regression model could be written as:

imdb_rating=4.38-0.16*genreAnimation+0.55*Genre Arthouse & International-0.12*GenreComedy + 0.79*genreDocumentary+0.21*genreDrama-0.12*GenreHorror+0.55*GenreMusical & performing arts+0.16*Genre Mystery & suspense+0.01*Genreother-0.42*genre Science Fiction & Fantasy+0.004*runtime+0.002*critics_score+1.938e-06*imdb_num_votes

In the context of the data, this model predicts that, all else held constant, movies that are of the genre Arthouse and international, or musical and performing, are expected to have an imdb rating on average 0.55 higher than movies of other genres. All else held constant, model predicts that a movie with an additional score in critics_rating is expected, on average, to have an additional 0.002 imdb rating. All else held constant, the model also predicts movies having an additional imdb_num_vote will have a rating 1.938e-06 higher. 

Before we can use the models to predict audience scores or imdb_rating, the conditions for multiple linear regression models need to be checked.

Condition 1: Linear relationships between numerical explanatory variables and the response variable. Residual plot is used to check this condition. 

The numeric explanatory variable to be plotted in the 1st model is critics_score. The numerical variables in the 2nd model are runtime, critics_score, and imdb_num_votes. 


```{r}
#checking residuals vs.critics_score in model 1 
plot(mlr_fin$residuals, mlr_fin$critics_score)
```


```{r}
#checking residuals vs.runtime in model 2
plot(mlr2_fin$residuals, mlr2_fin$runtime)
```

```{r}
#checking residuals vs.critics_score in model 2
plot(mlr2_fin$residuals, mlr2_fin$critics_score)
```


```{r}
#checking residuals vs.imdb_num_votes in model 2
plot(mlr2_fin$residuals, mlr2_fin$imdb_num_votes)
```


The above plots show residuals randomly scattered around zero. Therefore the first condition for multiple linear regression is met. 


2. Condition 2, nearly normal residuals with mean 0. Histogram and normal probability plot are used. 

```{r}
#check residuals in model 1
hist( mlr_fin$residuals) 
```

```{r}
#using normal probability plot to check the distribution of the residuals in model 1
qqnorm(mlr_fin$residuals)
qqline(mlr_fin$residuals)
```


As shown in the plots above, the distribution of the residuals in model 1 follow a nearly normal distribution. Now let's check model 2.

```{r}
hist(mlr2_fin$residuals)
```


```{r}
qqnorm(mlr2_fin$residuals)
qqline(mlr2_fin$residuals)
```


As shown above, the distribution of the residuals in model 2 is not exactly normal. 

Condition 3: constant variability of residuals. Residual plots of residuals vs. predicated values are used. 

```{r}
#residuals plot for model 1
plot(mlr_fin$residuals ~ mlr_fin$fitted)
plot(abs(mlr_fin$residuals) ~ mlr_fin$fitted)
```

```{r}
#residuals plot for model 2
plot(mlr2_fin$residuals ~ mlr2_fin$fitted)
plot(abs(mlr2_fin$residuals) ~ mlr2_fin$fitted)
```


The above plots show a constant variability of residuals in model 1. In model 2, there seems to be a slightly larger variability for predicted ratings below 6 than those above 6. 


4. Independent residuals. This condition can be checked using residuals vs.order of data selection. 
```{r}
#residuals in model 1
plot( mlr_fin$residuals)
```


```{r}
#residuals in model 2
plot(mlr2_fin$residuals)
```



The plots above shows that there is no pattern to the residuals in both models. It is therefore reasonable to consider the residuals independent from one another in both models. 

Overall, based on the diagnostic plots, model 1 can be considered a reliable multiple linear regression model to predict audience_scores. Model 2 can still be useful but is not as reliable a model to predict imdb_rating of movies. Model 2 probably can be improved if the outlier observations in imdb_rating are removed or treated separately. 

* * *

## Part 5: Prediction

Let's use the two models to predict the audience_score and imdb_rating of a 2016 movie: The girl on the train. 

The movie has 165,696 votes on imdb. Its runtime is 112 minutes and genre is mystery. Its critics score on Rotten Tomatoes is 44, and the movie hasn't been nominated for an Oscar best picture. Information is found on https://www.rottentomatoes.com/m/the_girl_on_the_train_2016 and https://www.imdb.com/title/tt3631112/

```{r}
#create new data.frame containing the information about the movie
newmovie<-data.frame(genre="Mystery & Suspense", runtime=112, imdb_num_votes=165696, critics_score=44, best_pic_nom="no" )
newmovie
```


```{r}
#predict the audience_score using model 1 mlr_fin
predict(mlr_fin, newmovie)
#quantifying uncertainty around prediction using prediction interval
predict(mlr_fin, newmovie, interval="prediction", level=0.95)
```

The audience score on Rotten Tomato is 49, which is pretty close to the predicted score using the model, 50.75111. Using the prediction interval, we are 95% confident that the model predicts that a movie having the genre of mystery and suspense, a critics score of 44, and not been nominated for Oscar best picture award, is expected to have an audience score between 23 to 78. 

Now let's use model 2 to predict the imdb_rating of this movie. 

```{r}
predict(mlr2_fin, newmovie)
#quantifying the uncertainty around the prediction using prediction interval. 
predict(mlr2_fin, newmovie, interval="prediction", level=0.95)
```


The imdb_rating for the movie on imdb site is 6.5, close to the predicted rating, 6.39. Using the prediction interval, the model predicts, with 95% confidence, that a movie of the genre of mystery and suspense, length of 112 minutes, critics score of 44, and having 165696 number of votes, is expected to have an imdb rating between 5.1 to 7.7.


* * *

## Part 6: Conclusion

To conclude, it's interesting to see that the popularity of movies as shown on two websites, imdb and Rotten Tomatoes, slightly differ in terms of their significant predictors. While genre seems to be significant factor in movie popularity for both websites, movies of which specific genres tend to more popular than others differ. If popularity is the goal, the models predict that it might be safer to produce and purchase Arthouse movies or musicals, and horror, based on the first model, may not be the best bet. 

Whether the movie has been nominated for an Oscar best picture is shown in model 1 to be a significant predictor for popularity audience, which is reasonable since the nomination comes from the audience after all. However, whether the movie has won the Oscar or not doesn't seem to be a significant predictor. It's also interesting to see that runtime is a significant predictor for ratings on IMDB.

Finally, it needs to be noted that, the value approach was used in the model selection in this report. Therefore, the models may not be the ones that have the best prediction accuracy. This can be shown in the R-squared and adjusted R-squared in the models, which is just above 50% and 60% respectively. If prediction accuracy is the goal, the R-squared approached could be used. 
